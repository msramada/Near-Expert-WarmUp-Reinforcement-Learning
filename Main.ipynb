{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from IPython import display\n",
    "import control\n",
    "from collections import deque\n",
    "import pickle\n",
    "from Tools.ExtKalmanFilter import *\n",
    "from Tools.sqrtm import sqrtm\n",
    "from Tools.Models_Buffers_More import *\n",
    "from RL_algorithms.DDPG import ddpg\n",
    "from RL_algorithms.PPO import ppo\n",
    "from ExampleModels.Dynamics_dbl_integrators import *\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "deviceCPU = torch.device(\"cpu\")\n",
    "print(device, deviceCPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures parameters\n",
    "saveFigs = True\n",
    "FigDim1 = 3.0\n",
    "FigDim2 = 1.5\n",
    "\n",
    "# Noise parameters Cov(w), Cov(v), respectively\n",
    "Q = 1 * torch.diag(1.0 * torch.ones(rx,)) #Filter covariance (typically is larger than true)\n",
    "Q_true = 0.1 * Q\n",
    "Q_true[2:rx, 2:rx] = 0.0                         #True covariance\n",
    "R = 0.1 * torch.diag(1.0 * torch.ones(ry,))\n",
    "\n",
    "# Linearized dynamic system:\n",
    "x_linearization = torch.tensor([[0, 0, 1, 1, 1, 1]], dtype = torch.float64).T\n",
    "A, B = torch.autograd.functional.jacobian(stateDynamics,\n",
    "                         inputs=(x_linearization, torch.zeros(ru, 1)))\n",
    "A = torch.atleast_2d(A.squeeze())\n",
    "B = torch.atleast_2d(B.squeeze()).T\n",
    "\n",
    "# LQR design params\n",
    "Q_lqr = torch.diag(1.0 * torch.ones(rx,))\n",
    "Q_lqr[2:rx, 2:rx] = 0.0\n",
    "R_lqr = torch.diag(1.0 * torch.ones(ru,))\n",
    "Klqr, _, _ = control.dlqr(torch.tensor([[1, 1],[0, 1]]), torch.tensor([[0, 1]]).T, torch.tensor([[1,0],[0,1]]), torch.tensor([1]))\n",
    "#Klqr, _, _ = control.dlqr(A, B, Q_lqr, R_lqr)\n",
    "Klqr = - torch.from_numpy(Klqr)\n",
    "Klqr = torch.cat((Klqr,torch.zeros(1,3)), dim=1)\n",
    "Klqr_hyperstate = torch.cat((Klqr,torch.zeros(1,rx)), dim=1)\n",
    "# Define model object to be used by the extended Kalman filter\n",
    "model = Model(stateDynamics, measurementDynamics, f_Jacobian, g_Jacobian, Q, R)\n",
    "\n",
    "# Define information state: (mean, covariance), from an extended Kalman Fitler.\n",
    "x0=torch.randn(rx, 1)\n",
    "P0=torch.diag(torch.ones(rx,))\n",
    "Hyperstate = Extended_KF(x0, P0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Val_NN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     RL_agent \u001b[39m=\u001b[39m ddpg(rx \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m,ru, hidden_dim, critic_lr, actor_lr, buffer_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m \u001b[39m64\u001b[39m, tau\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m, gamma\u001b[39m=\u001b[39mgamma, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m using_PPO:\n\u001b[1;32m---> 13\u001b[0m     RL_agent \u001b[39m=\u001b[39m ppo(ru, rx \u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m, hidden_dim, critic_lr, actor_lr, action_std, device)\n\u001b[0;32m     14\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m5000\u001b[39m\n\u001b[0;32m     15\u001b[0m miniBatch \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\msram\\OneDrive\\Documents\\GitHub\\Near-Expert-WarmUp-Reinforcement-Learning\\RL_algorithms\\PPO.py:11\u001b[0m, in \u001b[0;36mppo.__init__\u001b[1;34m(self, action_dim, state_dim, hidden_dim, val_lr, actor_lr, action_std, device, Buffer_size, gamma, lambda0, eps_clip, max_grad_norm, beta)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, action_dim, state_dim, hidden_dim, val_lr, actor_lr, action_std, device, \n\u001b[0;32m      9\u001b[0m              Buffer_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m\u001b[39m*\u001b[39m\u001b[39m32\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m, lambda0\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m, eps_clip\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, max_grad_norm \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m, beta\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[0;32m     10\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mActor \u001b[39m=\u001b[39m Actor_NN(state_dim, hidden_dim, action_dim)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mValue \u001b[39m=\u001b[39m Val_NN(state_dim, hidden_dim, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mReplayBuffer \u001b[39m=\u001b[39m ReplayBuffer(Buffer_size, device)\n\u001b[0;32m     13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_lr \u001b[39m=\u001b[39m val_lr\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Val_NN' is not defined"
     ]
    }
   ],
   "source": [
    "# Reinforcement learning infastructure and warmstart\n",
    "hidden_dim = 64\n",
    "actor_lr = 1e-5# * 10\n",
    "critic_lr = actor_lr * 10# * 100\n",
    "batch_size = 32#Horizon_Length# * RollOuts\n",
    "gamma = 0.95\n",
    "action_std = torch.tensor([[1.0]])\n",
    "using_DDPG = False\n",
    "using_PPO = True\n",
    "if using_DDPG:\n",
    "    RL_agent = ddpg(rx * 2,ru, hidden_dim, critic_lr, actor_lr, buffer_size = batch_size * 64, tau=0.05, gamma=gamma, device=device)\n",
    "if using_PPO:\n",
    "    RL_agent = ppo(ru, rx * 2, hidden_dim, critic_lr, actor_lr, action_std, device)\n",
    "epochs = 5000\n",
    "miniBatch = 64\n",
    "for j in range(epochs):\n",
    "    training_states = 50 * (torch.rand(miniBatch, rx * 2)-0.5)\n",
    "    training_states[:, 2:rx] = 1 + 0.2 * torch.randn(miniBatch, 3)\n",
    "    training_states[:, rx: 2*rx] = 5 * torch.rand(miniBatch, rx)\n",
    "    target_actions = training_states @ Klqr_hyperstate.T\n",
    "    RL_agent.warmstart_actor(training_states, target_actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Horizon_Length = 64\n",
    "epochs = 8\n",
    "NumberOf_Episodes = 1000\n",
    "actor_lr = 1e-5# * 10\n",
    "critic_lr = actor_lr * 10# * 100\n",
    "RL_agent.actor_lr = actor_lr\n",
    "RL_agent.critic_lr = critic_lr\n",
    "RollOuts = 1\n",
    "EffectiveHorizonLength = (1 - gamma ** Horizon_Length) / (1 - gamma)\n",
    "max_stageCost = 5e3\n",
    "rewards = []\n",
    "avg_rewards = []\n",
    "fig= plt.figure(figsize=(FigDim1,3 * FigDim2))\n",
    "plt.style.use('ggplot')\n",
    "plt.rc('xtick', labelsize=7) #fontsize of the x tick labels\n",
    "plt.rc('ytick', labelsize=7) #fontsize of the y tick labels\n",
    "plt.xlabel('episode number', fontsize=9)  \n",
    "plt.ylabel('normalized reward', fontsize=9) \n",
    "\n",
    "for episode in range(NumberOf_Episodes):\n",
    "    episode_reward = 0\n",
    "    xx_rec = []\n",
    "    Exploration_std = (NumberOf_Episodes - episode) / NumberOf_Episodes\n",
    "    for trail in range(RollOuts):\n",
    "        # Initial conditions of this rollout\n",
    "        Cov0 = 0.5 * torch.diag(torch.rand(rx,))\n",
    "        #Cov0[1,1] = 0.5 * torch.rand(1,)\n",
    "        x0 = 5 * torch.randn(rx, 1)\n",
    "        x0[2:5] = 1 + 0.2 * (torch.rand(3, 1)-0.5)\n",
    "        Hyperstate.ChangeInitialStates(x0, Cov0)\n",
    "        \n",
    "        # Horizon of this rollout\n",
    "        true_state = Hyperstate.Mean + sqrtm(Hyperstate.Covariance) @ torch.randn(rx, 1) #we might need next t-step\n",
    "        for k in range(Horizon_Length):\n",
    "            state = torch.cat((Hyperstate.Mean.T, torch.diag(Hyperstate.Covariance).unsqueeze(0)), dim=1).to(device)\n",
    "            action = RL_agent.get_action(state)\n",
    "            action = Exploration_std * torch.randn(ru,) + action\n",
    "            action = action.to(deviceCPU)\n",
    "            action = action.T\n",
    "            # Adding some exploration effort\n",
    "            action = torch.atleast_2d(action).detach()\n",
    "            # Create a transition\n",
    "            \n",
    "            true_state_Plus = stateDynamics(true_state, action) + sqrtm(Q_true) @ torch.randn(rx, 1)\n",
    "            measurement_Plus_realization = measurementDynamics(true_state_Plus, action) + sqrtm(R) @ torch.randn(ry, 1)\n",
    "            # reward function\n",
    "            reward = rewardFunction1(Hyperstate.Mean, Hyperstate.Covariance, action, Q_lqr, R_lqr, max_stageCost, true_state)\n",
    "            Test_Magnitudes = torch.cat((state, action), dim=1)\n",
    "            if ((Test_Magnitudes.abs() > 1e20).sum() > 0):\n",
    "                reward = -100\n",
    "                print('Failure')\n",
    "                break\n",
    "            # Advance to next hyperstate\n",
    "            Hyperstate.ApplyEKF(action, measurement_Plus_realization)\n",
    "            new_state = torch.cat((Hyperstate.Mean.T, torch.diag(Hyperstate.Covariance).unsqueeze(0)), dim=1)\n",
    "            RL_agent.ReplayBuffer.push(state, action.T, reward, new_state, torch.tensor([[k]]))\n",
    "            state = new_state\n",
    "            episode_reward += reward * gamma ** k\n",
    "            true_state = true_state_Plus\n",
    "            xx_rec.append(Hyperstate.Mean[0].detach().numpy())\n",
    "    if using_DDPG:\n",
    "        if len(RL_agent.Buffer)>= batch_size:\n",
    "            for jjj in  range(8):\n",
    "                RL_agent.train_critic(batch_size)\n",
    "            for jjj in  range(1):\n",
    "                RL_agent.train_actor(batch_size*1)\n",
    "    if using_PPO:\n",
    "        RL_agent.train(epochs=epochs)\n",
    "    \n",
    "    EffectiveHorizonLength = (1 - gamma ** k) / (1 - gamma)\n",
    "    rewards.append(episode_reward.numpy() / (EffectiveHorizonLength * RollOuts))\n",
    "    avg_rewards.append(np.mean(rewards[-5:]))\n",
    "\n",
    "    Hyperstate.printem()\n",
    "    if (True):\n",
    "        plt.clf()\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(avg_rewards, 'r')\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(xx_rec)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "fig= plt.figure(figsize=(FigDim1,FigDim2))\n",
    "plt.rc('xtick', labelsize=7) #fontsize of the x tick labels\n",
    "plt.rc('ytick', labelsize=7) #fontsize of the y tick labels\n",
    "plt.xlabel('episode number', fontsize=9)  \n",
    "plt.ylabel('normalized reward', fontsize=9) \n",
    "plt.plot(avg_rewards)\n",
    "plt.ylim([0.0, 1])\n",
    "if saveFigs==True:\n",
    "    plt.savefig('Figures/Example1_avgReward.pdf',bbox_inches =\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "K = RL_agent.get_action#get_action\n",
    "\n",
    "with open('Data/StateFeedbackGainFunction.pkl', 'wb') as outp:\n",
    "    pickle.dump(K, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open('StateFeedbackGainFunction.pkl', 'rb') as inp:\n",
    "    #K1 = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50\n",
    "\n",
    "StateRec = deque([])\n",
    "CovRec = deque([])\n",
    "ActionRec = deque([])\n",
    "StateRec_true = deque([])\n",
    "\n",
    "Cov0 = 0.5 * torch.diag(torch.rand(rx,))\n",
    "#Cov0[1,1] = 0.5 * torch.rand(1,)\n",
    "x0 = 5 * torch.randn(rx, 1)\n",
    "x0[2:5] = 1 + 0.2 * (torch.rand(3, 1)-0.5)\n",
    "print(x0)\n",
    "Hyperstate.ChangeInitialStates(x0, Cov0)\n",
    "true_state = Hyperstate.Mean + sqrtm(Hyperstate.Covariance) @ torch.randn(rx, 1) #we might need next t-step\n",
    "\n",
    "\n",
    "for k in range(T):\n",
    "    state = torch.cat((Hyperstate.Mean.T, torch.diag(Hyperstate.Covariance).unsqueeze(0)), dim=1)\n",
    "    action = K(state)\n",
    "    action = torch.atleast_2d(action.detach()).T\n",
    "    true_state_Plus = stateDynamics(true_state, action) + sqrtm(Q_true) @ torch.randn(rx, 1)\n",
    "    measurement_Plus_realization = measurementDynamics(true_state_Plus, action) + sqrtm(R) @ torch.randn(ry, 1)\n",
    "    StateRec.append(Hyperstate.Mean)\n",
    "    CovRec.append(Hyperstate.Covariance)\n",
    "    ActionRec.append(action)\n",
    "    StateRec_true.append(true_state)\n",
    "    Hyperstate.ApplyEKF(action, measurement_Plus_realization)\n",
    "    true_state = true_state_Plus\n",
    "x = np.ones((rx, T))\n",
    "x_true = np.ones((rx, T))\n",
    "P = np.ones((rx ** 2, T))\n",
    "for i in range(T):\n",
    "    x[:,i] = StateRec[i].detach().numpy().squeeze()\n",
    "    x_true[:,i] = StateRec_true[i].detach().numpy().squeeze()\n",
    "    P[:,i] = CovRec[i].detach().numpy().squeeze().flatten()\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig= plt.figure(figsize=(FigDim1,FigDim2))\n",
    "plt.rc('xtick', labelsize=7) #fontsize of the x tick labels\n",
    "plt.rc('ytick', labelsize=7) #fontsize of the y tick labels\n",
    "plt.xlabel('$k$', fontsize=9)  \n",
    "plt.ylabel('magnitude', fontsize=9) \n",
    "\n",
    "k=56\n",
    "plt.plot(range(T), x[0,:], color=list(colors.cnames)[k+4])\n",
    "plt.plot(range(T), P[0,:], color=list(colors.cnames)[k+2])\n",
    "plt.plot(range(T), x_true[0,:], linewidth = 1, color=list(colors.cnames)[k+3])\n",
    "\n",
    "if saveFigs==True:\n",
    "   plt.savefig('Figures/Example1_ReinfLearn.pdf',bbox_inches =\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50\n",
    "\n",
    "StateRec = deque([])\n",
    "ActionRec = deque([])\n",
    "StateRec_true = deque([])\n",
    "#Cov0 = 10 * torch.diag(torch.rand(rx,))\n",
    "#x0 = 10 * torch.randn(rx, 1)\n",
    "Hyperstate.ChangeInitialStates(x0, Cov0)\n",
    "true_state = Hyperstate.Mean + sqrtm(Hyperstate.Covariance) @ torch.randn(rx, 1) #we might need next t-step\n",
    "\n",
    "for k in range(T):\n",
    "    action = Klqr @ Hyperstate.Mean\n",
    "    action = torch.atleast_2d(action.detach())\n",
    "    true_state_Plus = stateDynamics(true_state, action) + sqrtm(Q_true) @ torch.randn(rx, 1)\n",
    "    measurement_Plus_realization = measurementDynamics(true_state_Plus, action) + sqrtm(R) @ torch.randn(ry, 1)\n",
    "    StateRec.append(Hyperstate.Mean)\n",
    "    ActionRec.append(action)\n",
    "    StateRec_true.append(true_state)\n",
    "    Hyperstate.ApplyEKF(action, measurement_Plus_realization)\n",
    "    true_state = true_state_Plus\n",
    "x = np.ones((rx, T))\n",
    "x_true = np.ones((rx, T))\n",
    "for i in range(T):\n",
    "    x[:,i] = StateRec[i].detach().numpy().squeeze()\n",
    "    x_true[:,i] = StateRec_true[i].detach().numpy().squeeze()\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig= plt.figure(figsize=(FigDim1,FigDim2))\n",
    "plt.rc('xtick', labelsize=7) #fontsize of the x tick labels\n",
    "plt.rc('ytick', labelsize=7) #fontsize of the y tick labels\n",
    "plt.xlabel('$k$', fontsize=9)  \n",
    "plt.ylabel('magnitude', fontsize=9) \n",
    "\n",
    "k=56\n",
    "plt.plot(range(T), x[0,:], color=list(colors.cnames)[k+4])\n",
    "#plt.plot(range(T), P, color=list(colors.cnames)[k+2])\n",
    "plt.plot(range(T), x_true[0,:], linewidth = 1, color=list(colors.cnames)[k+3])\n",
    "\n",
    "if saveFigs==True:\n",
    "   plt.savefig('Figures/Example1_ReinfLearn.pdf',bbox_inches =\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(StateRec[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.tensor([[1.0],[2.0]])\n",
    "x = torch.tensor([[5.0, 1.0, 3.2]]).T\n",
    "print(x[0])\n",
    "print(stateDynamics(x, u))\n",
    "print(f_Jacobian(x, u).trace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[0.9, 0, 0],\n",
    "                  [-0.05, 0.8, 0],\n",
    "                  [-0.2, 0, 0.95]])\n",
    "L, V = torch.linalg.eig(A)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.diag(Hyperstate.Covariance).unsqueeze(0))\n",
    "print(Hyperstate.Mean.T)\n",
    "print(torch.cat((Hyperstate.Mean.T, torch.diag(Hyperstate.Covariance).unsqueeze(0)), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyperstate.Covariance = Hyperstate.Covariance + -0.001\n",
    "print(Hyperstate.Covariance)\n",
    "sqrtm(Hyperstate.Covariance @ Hyperstate.Covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx = 3\n",
    "torch.zeros(rx, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
